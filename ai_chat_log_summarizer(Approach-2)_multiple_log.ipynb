{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7262e3",
   "metadata": {},
   "source": [
    "# Approach-2 (TF-IDF, NLTK) + Multiple AI Chat Log Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95212ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e9e988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chatlog_health.txt...\n",
      "Summary:\n",
      "- The conversation had 3 exchanges.\n",
      "- The user asked mainly about healthy eating and tip healthy eating.\n",
      "- Most common keywords: healthy eating, tip healthy eating, tip healthy, water drink daily, drink daily\n",
      "\n",
      "Processing chatlog_math.txt...\n",
      "Summary:\n",
      "- The conversation had 3 exchanges.\n",
      "- The user asked mainly about derivative used and help today.\n",
      "- Most common keywords: derivative used, help today, explain calculus, used determine rate, change slope\n",
      "\n",
      "Processing chatlog_travel.txt...\n",
      "Summary:\n",
      "- The conversation had 3 exchanges.\n",
      "- The user asked mainly about help travel plan and travel plan.\n",
      "- Most common keywords: help travel plan, travel plan, help travel, visit paris, best time\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder_path = \"chatlogs_folder\" \n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(f\"\\nProcessing {filename}...\")\n",
    "        with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # --- Chat Log Parsing ---\n",
    "        messages = []\n",
    "        current_speaker = None\n",
    "        current_message = \"\"\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"User:\"):\n",
    "                if current_speaker is not None:\n",
    "                    messages.append((current_speaker, current_message.strip()))\n",
    "                current_speaker = \"User\"\n",
    "                current_message = line[len(\"User: \"):].strip()\n",
    "            elif line.startswith(\"AI: \"):\n",
    "                if current_speaker is not None:\n",
    "                    messages.append((current_speaker, current_message.strip()))\n",
    "                current_speaker = \"AI\"\n",
    "                current_message = line[len(\"AI: \"):].strip()\n",
    "            else:\n",
    "                current_message += \" \" + line\n",
    "        if current_speaker is not None:\n",
    "            messages.append((current_speaker, current_message.strip()))\n",
    "\n",
    "        # --- Lemmatization ---\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        custom_stopwords = {\"hi\", \"hello\", \"hey\", \"thanks\", \"thank\", \"please\", \"ok\", \"okay\", \"sure\", \"yes\", \"no\", \"maybe\", \"let's\", \"let us\"}\n",
    "        stop_words.update(custom_stopwords)\n",
    "        lemmatized_docs = []\n",
    "        for msg in messages:\n",
    "            tokens = word_tokenize(msg[1])\n",
    "            lemmatized = ' '.join(\n",
    "                lemmatizer.lemmatize(w.lower())\n",
    "                for w in tokens\n",
    "                if w.isalpha() and w.lower() not in stop_words\n",
    "            )\n",
    "            lemmatized_docs.append(lemmatized)\n",
    "\n",
    "        # --- TF-IDF ---\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(2, 3))\n",
    "        tfidf_matrix = vectorizer.fit_transform(lemmatized_docs)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        scores = tfidf_matrix.sum(axis=0).A1\n",
    "        top_indices = scores.argsort()[-5:][::-1]\n",
    "        top_keywords = [feature_names[i] for i in top_indices]\n",
    "\n",
    "        # --- Message Statistics ---\n",
    "        user_messages = [msg for msg in messages if msg[0] == \"User\"]\n",
    "        ai_messages = [msg for msg in messages if msg[0] == \"AI\"]\n",
    "        exchanges_count = min(len(user_messages), len(ai_messages))\n",
    "        main_topic = top_keywords[0] if top_keywords else \"No main topic found\"\n",
    "\n",
    "        # --- Print the summary ---\n",
    "        print(\"Summary:\")\n",
    "        print(f\"- The conversation had {exchanges_count} exchanges.\")\n",
    "        print(f\"- The user asked mainly about {main_topic} and {top_keywords[1] if len(top_keywords) > 1 else ''}.\")\n",
    "        print(f\"- Most common keywords: {', '.join(top_keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b594c6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
